{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code of using HKUST Azure OpenAI API with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll find the completed sample code. However, if you're looking for a more detailed understanding and want to follow along step by step with thorough explanations, we highly recommend you to read our article below:\n",
    "\n",
    "👉 https://digitalhumanities.hkust.edu.hk/tutorials/how-to-use-hkust-azure-openai-api-key-with-python-with-sample-code-and-use-case-examples/\n",
    "\n",
    "The article provides comprehensive explanations and instructions that will help you grasp the underlying concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use your OpenAI API key in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 6 Nov 2023, [a new version of OpenAI](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) is released. It is important to note that this version upgrade from the previous version 0.28.1 to version 1.x is a breaking change. The code for the OpenAI Python API library differs between these two versions. Below is the sample code of the respective code snippets for both version 0.28.1 and version 1.x.\n",
    "\n",
    "While the newly released GPT-4 Turbo model undoubtedly offers enhanced capabilities and power, we have chosen to utilize the gpt-35-turbo model for demonstration here. Rest assured, even though it is a slightly older version, it still delivers impressive performance and serves our demonstration needs.\n",
    "\n",
    "We encourage you to explore the potential of GPT-4 Turbo yourself once Azure OpenAI support it.\n",
    "\n",
    "+ Details of different models: https://platform.openai.com/docs/models\n",
    "+ The pricing details for different models per 1,000 tokens can be found here (default: USD$):\n",
    "https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI version 0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Parameters\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://hkust.azure-api.net\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "openai.api_key = \"<your openai api key>\" #put your api key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to put your API key in a separate configuration file like `.env` for security reasons. This practice can separate your sensitive credentials from your codebase and ensure that they are not exposed if the code is shared or made public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may use the following code if you would like to store your api key in the .env file\n",
    "# Content in .env file ---->  OPENAI_API_KEY=<your openai api key>\n",
    "\n",
    "# If you are using jupyterlab and couldn’t locate the key in .env file. Here is an alternative way: https://gargankush.medium.com/storing-api-keys-as-environmental-variable-for-windows-linux-and-mac-and-accessing-it-through-974ba7c5109f\n",
    "\n",
    "'''\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('.env')\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def get_response(message, instruction):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine = 'gpt-35-turbo',\n",
    "        temperature = 1,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # print token usage\n",
    "    print(response.usage)\n",
    "    # return the response\n",
    "    return response.choices[0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt_tokens\": 26,\n",
      "  \"completion_tokens\": 39,\n",
      "  \"total_tokens\": 65\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Greetings! I am but a humble assistant, at your service. My tongue doth wag like the great bard Shakespeare, so speaketh thy commands and I shall bend my efforts to fulfill them.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"Who are you?\", \"You are an assistant that speaks like Shakespeare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI version 1.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please restart the notebook if you have installed another version of openai before. Run the install openai command in a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Parameters\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://hkust.azure-api.net\",\n",
    "  api_version = \"2023-05-15\",\n",
    "  api_key = \"<your openai api key>\" #put your api key here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def get_response(message, instruction):\n",
    "    response = client.chat.completions.create(\n",
    "\t\tmodel = 'gpt-35-turbo',\n",
    "        temperature = 1,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # print token usage\n",
    "    print(response.usage)\n",
    "    # return the response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=26, prompt_tokens=26, total_tokens=52)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am but a humble assistant, good sir or lady, at your service. What may I assist thee with this fine day?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"Who are you?\", \"You are an assistant that speaks like Shakespeare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --upgrade\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Parameters\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://hkust.azure-api.net\",\n",
    "  api_version = \"2023-05-15\",\n",
    "  api_key = \"<your openai api key>\" #put your api key here\n",
    ")\n",
    "# Function\n",
    "def get_response(message, instruction):\n",
    "    response = client.chat.completions.create(\n",
    "\t\tmodel = 'gpt-35-turbo',\n",
    "        temperature = 1,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # print the response\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "    # return the response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text copied from https://library.hkust.edu.hk/events/conferences/ai-scholarly-commu-2023/\n",
    "\n",
    "Text = \"\"\"\n",
    "14 November 2023\n",
    "2:30 pm – 4:00 pm\n",
    "Search Engine and Large Language Models – Can they truly change the game?\n",
    "REGISTER\n",
    "Abstract\n",
    "Academic search engines are racing to incorporate the latest advancements brought about by Large Language models (LLMs) in terms of their ability to understand queries, extract information and directly generate answers. The first movers in this space were startup and challengers such as Elicit, Consensus.AI, Scite assistant, Scispace but they have recently been joined by established academic search engine provider like Elsevier’s Scopus and Digital Science’s Dimensions joining the fray with more to come.\n",
    "\n",
    "Using techniques like RAG (Retrieval augmented generation), this first wave of academic search engines hopes to combine search technology with generative AI by grounding the answers generated by LLMs using information context found by search engines, with the hope of reducing hallucinations. But is this enough?\n",
    "\n",
    "Join Aaron as he shares his experience testing and using these tools and his best guess on how these tools might develop in the future and their impact on research writing in the future.\n",
    "\n",
    "About the Speaker\n",
    "\n",
    "Aaron TayMr. Aaron Tay has been an academic librarian for over 10 years in Singapore and has worked in a variety of areas including library discovery, research support & bibliometrics. He is current Lead Data Services at the Singapore Management University Libraries and has been honoured for his contributions to the profession with a few awards including Library Association of Singapore (LAS) Professional Service Award, Congress of Southeast Asian Libraries (CONSAL) award (Silver) and Pacific Rim Research Library Alliance (PRRLA) , Karl Lo award.\n",
    "\n",
    "A past contributor to NMC horizon report (library edition), as well as a founding member of the Initiative for Open Abstracts, he has blended his interest in discovery and the evolving Scholarly ecosystem and has given talks on how AI/ML might change Scholarly communication. More recently, he has contributed to panels and given keynotes on the impact of AI and in particular large language models on academic libraries and institutions at conferences like CILIP, IATUL and more. He has been blogging at MusingsAboutLibrarianship.blogpost.com since 2009.\n",
    "\n",
    " \n",
    "\n",
    "15 November 2023\n",
    "4:00 pm – 5:30 pm\n",
    "Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\n",
    "REGISTER\n",
    "Abstract\n",
    "Screening thousands of research papers for a systematic review or meta-analysis can be overwhelming. The reality is that there simply isn’t enough time to read every single article.\n",
    "\n",
    "Join Prof. Dr. Rens van de Schoot as he introduces ASReview, a powerful free and open-source software for systematic reviewing, developed by his research team from Utrecht University. Rens will explain how active learning, a machine learning technique, can accelerate the step of manual screening process by saving up to 95% (!) of screening time. ASReview is more than just a tool; it’s a vibrant community of researchers, users, and developers worldwide, contributing to its open-source mission, and Rens will explain how you can join the movement towards fast, open, and transparent systematic reviews.\n",
    "\n",
    "About the Speaker\n",
    "\n",
    "prof rens van de schoot profile photoProf. Dr. Rens van de Schoot works as a full professor for ‘Statistics for Small Data Sets’ at Utrecht University in the Netherlands and as an extra-ordinary professor at North-West University in South Africa. He is also the program director of the research master ‘Methodology and Statistics for the Behavioural, Biomedical, and Social Sciences’. He is known for his many tutorials, checklists, and online (free) course materials in the areas of SEM and Bayesian statistics. Currently, his main research project is the community-driven and fully open-source project ASReview: AI-aided systematic reviewing using Active Learning. \n",
    "\n",
    " \n",
    "\n",
    "22 November 2023\n",
    "10:30 am – 12:00 pm\n",
    "Generative AI for Translational Scholarly Communication\n",
    "REGISTER\n",
    "Abstract\n",
    "Many valuable insights embedded in scientific publications are siloed and rarely translated into results that can directly benefit humans. These research-to-practice gaps impede the diffusion of innovation, undermine evidence-based decision making, and contribute to the disconnect between science and the public. Generative AI systems trained on decades of digitized scholarly publications and other human-produced texts are now capable of generating (mostly) high-quality and (sometimes) trustworthy text, images, and media. Applied in the context of scholarly communication, Generative AI can quickly summarize research findings, generate visual diagrams of scientific content, and simplify technical jargon. In essence, Generative AI has the potential to help tailor language, format, tone, and examples to make research more accessible, understandable, engaging, and useful for different audiences.  \n",
    "\n",
    "In this talk, I’ll discuss some uses of Generative AI in these contexts as well as challenges towards realizing the potential of these models, e.g., how to effectively design generated translational science communication artifacts, incorporate human feedback in the process, and mitigate the generation of harmful, misleading, or false information. Scholarly communication is undergoing a major transformation with the emergence of these new tools. By using them safely, we can help bridge the research-to-practice gap and maximize the impacts of scientific discovery. \n",
    "\n",
    "About the Speaker\n",
    "\n",
    "Dr Lucy Lu Wang profile photoLucy Lu Wang is an Assistant Professor at the University of Washington Information School. Her research focuses on how to build better AI and NLP systems for extracting and understanding information from scientific texts; for example, can we create systems that leverage up-to-date literature to help us make better and more data-driven healthcare decisions, or design document understanding models that can improve the readability of scientific texts for people who are blind and low vision. Lucy’s work on supplement interaction detection, gender trends in academic publishing, COVID-19 datasets, and document understanding has been featured in Geekwire, Boing Boing, Axios, VentureBeat, and the New York Times. Prior to joining the UW, she was a Young Investigator at the Allen Institute for AI, and she received her PhD in Biomedical Informatics and Medical Education from the University of Washington.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1:\n",
      "Name: Aaron Tay\n",
      "University: Singapore Management University\n",
      "Specialist: Academic librarian with a focus on library discovery, research support, and bibliometrics.\n",
      "Presentation Title: Search Engine and Large Language Models – Can they truly change the game?\n",
      "\n",
      "Speaker 2:\n",
      "Name: Prof. Dr. Rens van de Schoot\n",
      "University: Utrecht University (Netherlands) and North-West University (South Africa)\n",
      "Specialist: Full Professor for ‘Statistics for Small Data Sets’ and Program Director for the research master ‘Methodology and Statistics for the Behavioural, Biomedical, and Social Sciences’\n",
      "Presentation Title: Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\n",
      "\n",
      "Speaker 3:\n",
      "Name: Dr. Lucy Lu Wang\n",
      "University: University of Washington Information School\n",
      "Specialist: Assistant Professor in AI and NLP, with a focus on extracting and understanding information from scientific texts\n",
      "Presentation Title: Generative AI for Translational Scholarly Communication\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Speaker 1:\\nName: Aaron Tay\\nUniversity: Singapore Management University\\nSpecialist: Academic librarian with a focus on library discovery, research support, and bibliometrics.\\nPresentation Title: Search Engine and Large Language Models – Can they truly change the game?\\n\\nSpeaker 2:\\nName: Prof. Dr. Rens van de Schoot\\nUniversity: Utrecht University (Netherlands) and North-West University (South Africa)\\nSpecialist: Full Professor for ‘Statistics for Small Data Sets’ and Program Director for the research master ‘Methodology and Statistics for the Behavioural, Biomedical, and Social Sciences’\\nPresentation Title: Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\\n\\nSpeaker 3:\\nName: Dr. Lucy Lu Wang\\nUniversity: University of Washington Information School\\nSpecialist: Assistant Professor in AI and NLP, with a focus on extracting and understanding information from scientific texts\\nPresentation Title: Generative AI for Translational Scholarly Communication'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"You are a helpful assistant.\"\n",
    "\n",
    "get_response(f\"\"\"\n",
    "             Please extract each of the speaker name, their corresponding university, their specialist (within 20 words), and presentation title. \n",
    "             Text:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker Name,University,Specialist,Presentation Title\n",
      "Aaron Tay,Singapore Management University Libraries,Academic search engines and bibliometrics,\"Search Engine and Large Language Models – Can they truly change the game?\"\n",
      "Prof. Dr. Rens van de Schoot,Utrecht University,Statistics for Small Data Sets,\"Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\"\n",
      "Dr. Lucy Lu Wang,University of Washington Information School,Artificial Intelligence and Natural Language Processing,\"Generative AI for Translational Scholarly Communication\"\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are a helpful assistant.\"\n",
    "\n",
    "csv_result = get_response(f\"\"\"\n",
    "             Please extract each of the speaker name, their corresponding university, their specialist (within 20 words), and presentation title. Please return the results in csv format.\n",
    "             Text:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Speaker Name,University,Specialist,Presentation Title\\nAaron Tay,Singapore Management University Libraries,Academic search engines and bibliometrics,\"Search Engine and Large Language Models – Can they truly change the game?\"\\nProf. Dr. Rens van de Schoot,Utrecht University,Statistics for Small Data Sets,\"Saving Time and Sanity: Using active learning for systematic reviews and meta-analyses\"\\nDr. Lucy Lu Wang,University of Washington Information School,Artificial Intelligence and Natural Language Processing,\"Generative AI for Translational Scholarly Communication\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A csv file is created and saved in the same folder of this notebook.\n"
     ]
    }
   ],
   "source": [
    "# save the result to csv file\n",
    "import csv  \n",
    "with open('outputfile_NER-example.csv', 'w', encoding='UTF8') as f:\n",
    "    f.write(csv_result)    \n",
    "    print(\"A csv file is created and saved in the same folder of this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classified by sentiment: Positive, Negative, Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really enjoyed the movie. It was fantastic!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service at the restaurant was terrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The customer support was very helpful and resp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The shop was okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The product I bought was of poor quality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0      I really enjoyed the movie. It was fantastic!\n",
       "1        The service at the restaurant was terrible.\n",
       "2  The customer support was very helpful and resp...\n",
       "3                                 The shop was okay.\n",
       "4          The product I bought was of poor quality."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "sampledata = {\n",
    "    'Text': [\"I really enjoyed the movie. It was fantastic!\", \n",
    "             \"The service at the restaurant was terrible.\", \n",
    "             \"The customer support was very helpful and responsive.\",\n",
    "             \"The shop was okay.\",\n",
    "             \"The product I bought was of poor quality.\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=sampledata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "negative\n",
      "positive\n",
      "neutral\n",
      "negative\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really enjoyed the movie. It was fantastic!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service at the restaurant was terrible.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The customer support was very helpful and resp...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The shop was okay.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The product I bought was of poor quality.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0      I really enjoyed the movie. It was fantastic!  positive\n",
       "1        The service at the restaurant was terrible.  negative\n",
       "2  The customer support was very helpful and resp...  positive\n",
       "3                                 The shop was okay.   neutral\n",
       "4          The product I bought was of poor quality.  negative"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the instruction for sentiment analysis\n",
    "instruction = \"Please analyze the sentiment of the following text. Only use the exact wording 'positive', 'negative', or 'neutral' in your response. Do not say any other irrelevant things, no punctuation.\"\n",
    "\n",
    "# Create a new column for sentiment\n",
    "df['Sentiment'] = \"\"\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the text from the 'Text' column\n",
    "    text = row['Text']\n",
    "    \n",
    "    # Get the sentiment response using the get_response function\n",
    "    sentiment = get_response(text, instruction)\n",
    "    \n",
    "    # Store the sentiment result in the 'Sentiment' column\n",
    "    df.at[index, 'Sentiment'] = sentiment\n",
    "\n",
    "# display the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classified by categories: Facilities, Services, Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best place to stay in UST.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our library is the best!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the ground floor of the library, i thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our library provide information and resources ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0                     The best place to stay in UST.\n",
       "1                           Our library is the best!\n",
       "2  I love the ground floor of the library, i thin...\n",
       "3  Our library provide information and resources ...\n",
       "4  服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...\n",
       "5      图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the comments that the library received in LibQUAL 2019 as sample data https://library.hkust.edu.hk/about-us/user-engagement/libqual2019/\n",
    "sampledata = {\n",
    "    'Text': [\"The best place to stay in UST.\",\n",
    "            \"Our library is the best!\",\n",
    "            \"I love the ground floor of the library, i think especially in the morning it is so peaceful and calm.\",\n",
    "            \"Our library provide information and resources for us, including materials and many classes about how to use them. I really thanks staffs of library. And further I hope that our library could provide more chance and resource of 3D printer and I really love it.\",\n",
    "            \"服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，活動種類繁多，可以讓我找到興趣，又能學習 新的東西。\",\n",
    "            \"图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。\"]\n",
    "            }\n",
    "\n",
    "libqual2019 = pd.DataFrame(data=sampledata)\n",
    "libqual2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "Cannot be classified.\n",
      "positive\n",
      "Cannot be classified.\n",
      "positive\n",
      "Facilities\n",
      "positive\n",
      "Services, Resources.\n",
      "positive\n",
      "Services, Facilities, Resources, Activities\n",
      "positive\n",
      "Services, Activities.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best place to stay in UST.</td>\n",
       "      <td>positive</td>\n",
       "      <td>Cannot be classified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our library is the best!</td>\n",
       "      <td>positive</td>\n",
       "      <td>Cannot be classified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love the ground floor of the library, i thin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Facilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our library provide information and resources ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Services, Resources.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Services, Facilities, Resources, Activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。</td>\n",
       "      <td>positive</td>\n",
       "      <td>Services, Activities.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment  \\\n",
       "0                     The best place to stay in UST.  positive   \n",
       "1                           Our library is the best!  positive   \n",
       "2  I love the ground floor of the library, i thin...  positive   \n",
       "3  Our library provide information and resources ...  positive   \n",
       "4  服務良好、環境非常舒適，可以讓我專心溫習、 資源豐富、職員亦很友善，樂意詳細回答我的問 題，...  positive   \n",
       "5      图书馆有很好的环境，也会提供各种活动和讲座帮助我们熟悉使用图书馆的资源，喜欢我们的图书馆。  positive   \n",
       "\n",
       "                                      Category  \n",
       "0                        Cannot be classified.  \n",
       "1                        Cannot be classified.  \n",
       "2                                   Facilities  \n",
       "3                         Services, Resources.  \n",
       "4  Services, Facilities, Resources, Activities  \n",
       "5                        Services, Activities.  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the instruction for sentiment analysis\n",
    "instruction1 = \"Please analyze the sentiment of the following text. Only use the exact wording 'positive', 'negative', or 'neutral' in your response. All lowercase. Do not say any other irrelevant things. Do not include full stop in your response.\"\n",
    "\n",
    "# Define the instruction for category classification\n",
    "instruction2 = \"\"\"\n",
    "Please classify the following text into these categories (exact wordings) 'Services', 'Facilities', 'Resources', 'Activities' or 'Cannot be classified' in your response. Do not say any other words. Remember only use these 4 categories in your response: 'Services', 'Facilities', 'Resources', 'Activities', 'Cannot be classified'. Use 'Cannot be classified' only when no categories can be assigned to the text.\n",
    "\"\"\"\n",
    "\n",
    "# Create new columns for sentiment and category\n",
    "libqual2019['Sentiment'] = \"\"\n",
    "libqual2019['Category'] = \"\"\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the text from the 'Text' column\n",
    "    text = row['Text']\n",
    "    \n",
    "    # Get the response using the get_response function\n",
    "    sentiment = get_response(text, instruction1)\n",
    "    category = get_response(text, instruction2)\n",
    "    \n",
    "    # Store the result in the column\n",
    "    libqual2019.at[index, 'Sentiment'] = sentiment\n",
    "    libqual2019.at[index, 'Category'] = category\n",
    "\n",
    "# display the updated DataFrame\n",
    "libqual2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Translation\n",
    "Example: classical Chinese (文言文) to vernacular Chinese (白話文)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "白話文：\n",
      "現在有一塊土地，長十五步，寬十六步，問這塊土地面積是多少？回答是一畝。\n",
      "\n",
      "還有一塊土地，長十二步，寬十四步，問這塊土地面積是多少？回答是一百六十八步。\n",
      "\n",
      "求土地面積的方法叫做「方田術」。可以用長和寬的步數相乘，得到土地的面積。如果要換算成畝，要用每畝二百四十步的面積去除，得到的數字就是畝數。一百畝等於一頃。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are an expert in Mathematics and proficient in classical Chinese.\"\n",
    "Text = \"\"\"\n",
    "今有田廣十五步，從十六步。問為田幾何？\t\t\n",
    "答曰：一畝。\n",
    "方田：又有田廣十二步，從十四步。問為田幾何？\t\t\n",
    "答曰：一百六十八步。\t\t\n",
    "方田術曰：廣從步數相乘得積步。以畝法二百四十步除之，即畝數。百畝為一頃。\n",
    "\"\"\"\n",
    "\n",
    "get_response(f\"\"\"\n",
    "             請把以下文言文翻譯成白話文：\n",
    "             文言文:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "首先，將題目翻譯成現代中文：現在有一塊田，長15步，寬16步，問這塊田是多大面積？ 答案是一畝。接著，又有一塊長12步，寬14步的田，問這塊田是多大面積？答案是一百六十八步。\n",
      "\n",
      "根據方田術，田的面積可以通過廣（長）和從（寬）的步數相乘而得。因此，第一塊田的面積為：\n",
      "\n",
      "長 × 寬 = 15 × 16 = 240 步\n",
      "\n",
      "根據一畝等於二百四十步的標準，這塊田的面積為一畝。\n",
      "\n",
      "同樣地，第二塊田的面積為：\n",
      "\n",
      "長 × 寬 = 12 × 14 = 168 步\n",
      "\n",
      "因此，第二塊田的面積為一百六十八步。\n",
      "\n",
      "最後，方田術還提到，將田地的面積除以二百四十步，就可以得到田的畝數。一頃等於一百畝。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"You are an expert in Mathematics and proficient in classical Chinese.\"\n",
    "Text = \"\"\"\n",
    "今有田廣十五步，從十六步。問為田幾何？\t\t\n",
    "答曰：一畝。\n",
    "方田：又有田廣十二步，從十四步。問為田幾何？\t\t\n",
    "答曰：一百六十八步。\t\t\n",
    "方田術曰：廣從步數相乘得積步。以畝法二百四十步除之，即畝數。百畝為一頃。\n",
    "\"\"\"\n",
    "\n",
    "get_response(f\"\"\"\n",
    "             請以數學公式逐步解釋以下文言文的內容:\n",
    "             文言文:{Text}\n",
    "            \"\"\", \n",
    "            instruction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
